{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('p37env': conda)",
   "metadata": {
    "interpreter": {
     "hash": "88d86be733a9a448ab4216705ec79d5515b5f923491cd63b6a11efb829921b60"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords, reuters\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "all_tweets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vaccine_csv():\n",
    "    df = pd.read_csv('./covidvaccine.csv')\n",
    "    df = df[['user_location','user_followers',\t'user_friends',\t'user_favourites',\t'user_verified',\t'date',\t'text',\t'source']]\n",
    "    # '18-08-2020 12:55'\n",
    "    # df.index = pd.to_datetime(df['date'])\n",
    "    # for managebility we remove the hashtag, but it might have a future use-case\n",
    "    # is_retweet is all False in this dataset\n",
    "    # df = df[df['is_retweet'] == False]\n",
    "    # df = df.dropna()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "time data '2020-10-01 10:24:56' does not match format '%d-%m-%Y %H:%M' (match)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/envs/p37env/lib/python3.7/site-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36m_convert_listlike_datetimes\u001b[0;34m(arg, format, name, tz, unit, errors, infer_datetime_format, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m                 \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime_to_datetime64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m                 \u001b[0mdta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatetimeArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtz_to_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/tslibs/conversion.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.conversion.datetime_to_datetime64\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Unrecognized value type: <class 'str'>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-b6f0edaf909c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#18-08-2020 12:55\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# df = df['date'].apply(lambda t: t.strftime('%D-%M-%Y %H:%M'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'xxx'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'%d-%m-%Y %H:%M'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m# df = df[df.date.isnull()]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/p37env/lib/python3.7/site-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_listlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mABCDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMutableMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/p37env/lib/python3.7/site-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36m_convert_listlike_datetimes\u001b[0;34m(arg, format, name, tz, unit, errors, infer_datetime_format, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[1;32m    452\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mDatetimeIndex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_simple_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/p37env/lib/python3.7/site-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36m_convert_listlike_datetimes\u001b[0;34m(arg, format, name, tz, unit, errors, infer_datetime_format, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m                     result, timezones = array_strptime(\n\u001b[0;32m--> 418\u001b[0;31m                         \u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexact\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m                     )\n\u001b[1;32m    420\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;34m\"%Z\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"%z\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/tslibs/strptime.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: time data '2020-10-01 10:24:56' does not match format '%d-%m-%Y %H:%M' (match)"
     ]
    }
   ],
   "source": [
    "# 354390\n",
    "# 354390\n",
    "# 216910\n",
    "df = load_vaccine_csv()\n",
    "# df.index = pd.to_datetime(df.date, errors='coerce')\n",
    "\n",
    "#18-08-2020 12:55\n",
    "# df = df['date'].apply(lambda t: t.strftime('%D-%M-%Y %H:%M'))\n",
    "df['xxx'] = pd.to_datetime(df['date'], format='%d-%m-%Y %H:%M')\n",
    "# df = df[df.date.isnull()]\n",
    "\n",
    "# df = df.sort_index()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.dropna()\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_vaccine_csv().dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to display data of each tweet \n",
    "def printtweetdata(n, ith_tweet): \n",
    "    print() \n",
    "    print(f\"Tweet {n}:\") \n",
    "    print(f\"Username:{ith_tweet[0]}\") \n",
    "    print(f\"Description:{ith_tweet[1]}\") \n",
    "    print(f\"Location:{ith_tweet[2]}\") \n",
    "    print(f\"Following Count:{ith_tweet[3]}\") \n",
    "    print(f\"Follower Count:{ith_tweet[4]}\") \n",
    "    print(f\"Total Tweets:{ith_tweet[5]}\") \n",
    "    print(f\"Retweet Count:{ith_tweet[6]}\") \n",
    "    print(f\"Tweet Text:{ith_tweet[7]}\") \n",
    "    print(f\"Hashtags Used:{ith_tweet[8]}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to perform data extraction \n",
    "def scrape(words, date_since, numtweet): \n",
    "      \n",
    "    # Creating DataFrame using pandas \n",
    "    db = pd.DataFrame(columns=['username', 'description', 'location', 'following', \n",
    "                               'followers', 'totaltweets', 'retweetcount', 'text', 'hashtags', 'created_at']) \n",
    "      \n",
    "    # We are using .Cursor() to search through twitter for the required tweets. \n",
    "    # The number of tweets can be restricted using .items(number of tweets) \n",
    "    tweets = tweepy.Cursor(api.search, q=words + \" -filter:retweets\", lang=\"en\", \n",
    "                           since=date_since, tweet_mode='extended').items(numtweet) \n",
    "     \n",
    "    list_tweets = []\n",
    "    while True:\n",
    "        try:\n",
    "            tweet = tweets.next()\n",
    "            print(tweet)\n",
    "            list_tweets.append(tweet)\n",
    "            # Insert into db\n",
    "        except tweepy.TweepError:\n",
    "            time.sleep(60 * 15)\n",
    "            continue\n",
    "        except StopIteration:\n",
    "            break\n",
    "    # .Cursor() returns an iterable object. Each item in  \n",
    "    # the iterator has various attributes that you can access to  \n",
    "    # get information about each tweet \n",
    "    # list_tweets = [tweet for tweet in tweets] \n",
    "      \n",
    "    # Counter to maintain Tweet Count \n",
    "    i = 1  \n",
    "      \n",
    "    # we will iterate over each tweet in the list for extracting information about each tweet \n",
    "    for tweet in list_tweets: \n",
    "        print(tweet)\n",
    "        username = tweet.user.screen_name \n",
    "        description = tweet.user.description \n",
    "        location = tweet.user.location \n",
    "        following = tweet.user.friends_count \n",
    "        followers = tweet.user.followers_count \n",
    "        totaltweets = tweet.user.statuses_count \n",
    "        retweetcount = tweet.retweet_count \n",
    "        created_at = tweet.created_at\n",
    "        hashtags = tweet.entities['hashtags'] \n",
    "          \n",
    "        # Retweets can be distinguished by a retweeted_status attribute, \n",
    "        # in case it is an invalid reference, except block will be executed \n",
    "        try: \n",
    "            text = tweet.retweeted_status.full_text \n",
    "            all_tweets.append(text)\n",
    "            # print(text)\n",
    "        except AttributeError: \n",
    "            text = tweet.full_text \n",
    "        hashtext = list() \n",
    "        for j in range(0, len(hashtags)): \n",
    "            hashtext.append(hashtags[j]['text']) \n",
    "          \n",
    "        # Here we are appending all the extracted information in the DataFrame \n",
    "        ith_tweet = [username, description, location, following, \n",
    "                     followers, totaltweets, retweetcount, text, hashtext, created_at] \n",
    "        db.loc[len(db)] = ith_tweet \n",
    "          \n",
    "        # Function call to print tweet data on screen \n",
    "        # printtweetdata(i, ith_tweet) \n",
    "        i = i+1\n",
    "    filename = 'scraped_tweets.csv'\n",
    "      \n",
    "    # we will save our database as a CSV file. \n",
    "    db.to_csv(filename) \n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    " # Enter your own credentials obtained  \n",
    "    # from your developer account \n",
    "consumer_key = \"SCOPBRKeG4nRCEa7XkMoqQ\"\n",
    "consumer_secret = \"RYInMkLiNyg0iKC3g89Y0f8g8kbNFSsNjZXNYBYILQ\"\n",
    "access_key = \"107275871-PtAa9t7OJX82IEbGjdOzaFMgeCWQyU8haNPhN4mD\"\n",
    "access_secret = \"Q691rTrztjXeYAP5FEA5kZWRhuFRtH8sq6msPgUeD1TzI\"\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret) \n",
    "auth.set_access_token(access_key, access_secret) \n",
    "api = tweepy.API(auth) \n",
    "    \n",
    "# Enter Hashtag and initial date \n",
    "# print(\"Enter Twitter HashTag to search for\") \n",
    "words = 'covid19vaccine' \n",
    "# print(\"Enter Date since The Tweets are required in yyyy-mm--dd\") \n",
    "date_since = '2016-01-01' \n",
    "    \n",
    "# number of tweets you want to extract in one run \n",
    "numtweet = 1000  \n",
    "df = scrape(words, date_since, numtweet) \n",
    "print('Scraping has completed!') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "df.describe()\n",
    "# ####input your credentials here\n",
    "# consumer_key = 'SCOPBRKeG4nRCEa7XkMoqQ'\n",
    "# consumer_secret = 'RYInMkLiNyg0iKC3g89Y0f8g8kbNFSsNjZXNYBYILQ'\n",
    "# access_token = '107275871-PtAa9t7OJX82IEbGjdOzaFMgeCWQyU8haNPhN4mD'\n",
    "# access_token_secret = 'Q691rTrztjXeYAP5FEA5kZWRhuFRtH8sq6msPgUeD1TzI'\n",
    "\n",
    "# auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "# auth.set_access_token(access_token, access_token_secret)\n",
    "# api = tweepy.API(auth,wait_on_rate_limit=True)\n",
    "# #####United Airlines\n",
    "# # Open/Create a file to append data\n",
    "# csvFile = open('ua.csv', 'a')\n",
    "# #Use csv Writer\n",
    "# csvWriter = csv.writer(csvFile)\n",
    "\n",
    "# all_tweets = []\n",
    "\n",
    "# for tweet in tweepy.Cursor(api.search,q=\"#dulux\",count=100,\n",
    "#                            lang=\"en\",\n",
    "#                            since=\"2015-11-03\").items():\n",
    "#     print (tweet.created_at, tweet.text)\n",
    "#     all_tweets.append(tweet.text)\n",
    "#     csvWriter.writerow([tweet.created_at, tweet.text.encode('utf-8')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(doc):\n",
    "    sw = set(stopwords.words('english'))\n",
    "    regex = re.compile(\"[^a-zA-Z ]\")\n",
    "    re_clean = regex.sub('', doc)\n",
    "    words = word_tokenize(re_clean)\n",
    "    lem = [lemmatizer.lemmatize(word) for word in words]\n",
    "    output = [word.lower() for word in lem if word.lower() not in sw]\n",
    "    return ' '.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets\n",
    "big_string = ' '.join(all_tweets)\n",
    "input_text = process_text(big_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(input_text)\n",
    "# plt.imshow(wc)\n",
    "plt.figure(figsize=(100,20))\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_bg(doc):\n",
    "    sw = set(stopwords.words('english'))\n",
    "    regex = re.compile(\"[^a-zA-Z ]\")\n",
    "    re_clean = regex.sub('', doc)\n",
    "    words = word_tokenize(re_clean)\n",
    "    lem = [lemmatizer.lemmatize(word) for word in words]\n",
    "    sw_words = [word.lower() for word in lem if word.lower() not in sw]\n",
    "    bigrams = ngrams(sw_words, 2)\n",
    "    output = ['_'.join(i) for i in bigrams]\n",
    "    return ' '.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "input_bigrams = process_text_bg(big_string)\n",
    "wc = WordCloud().generate(input_bigrams)\n",
    "plt.figure(figsize=(15,24))\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db"
   ]
  }
 ]
}